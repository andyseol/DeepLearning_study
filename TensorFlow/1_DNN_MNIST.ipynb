{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0248a9",
   "metadata": {},
   "source": [
    "# Deep Neural Network (DNN)\n",
    "- MNIST Database\n",
    "- Function: Sigmoid vs. ReLU func\n",
    "- Initialization: RandomNormal vs. Xavier\n",
    "- Dropout vs. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85eea2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical    # converts a class vector (integers) to binary class matrix\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from time import time\n",
    "import os\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f05667",
   "metadata": {},
   "source": [
    "### Checkpoint function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b464e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model, checkpoint_dir):\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt :\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        checkpoint = tf.train.Checkpoint(dnn=model)\n",
    "        checkpoint.restore(save_path=os.path.join(checkpoint_dir, ckpt_name))\n",
    "        counter = int(ckpt_name.split('-')[1])\n",
    "        print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "        return True, counter\n",
    "    else:\n",
    "        print(\" [*] Failed to find a checkpoint\")\n",
    "        return False, 0\n",
    "\n",
    "def check_folder(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6eedfa",
   "metadata": {},
   "source": [
    "### Data load & pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07bf096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist() :\n",
    "    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "    train_data = np.expand_dims(train_data, axis=-1)    # [N, 28, 28] -> [N, 28, 28, 1]\n",
    "    test_data = np.expand_dims(test_data, axis=-1)    # [N, 28, 28] -> [N, 28, 28, 1]\n",
    "\n",
    "    train_data, test_data = normalize(train_data, test_data)\n",
    "\n",
    "    train_labels = to_categorical(train_labels, 10)    # [N,] -> [N, 10]\n",
    "    test_labels = to_categorical(test_labels, 10)    # [N,] -> [N, 10]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def normalize(train_data, test_data):\n",
    "    train_data = train_data.astype(np.float32) / 255.0    # 명암이 0~255의 값으로 표현되므로\n",
    "    test_data = test_data.astype(np.float32) / 255.0\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc3bd0",
   "metadata": {},
   "source": [
    "### Performance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c151912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss: log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images, training=True)    # True: dropout on\n",
    "    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true=labels, y_pred=logits, from_logits=True))\n",
    "    return loss\n",
    "\n",
    "def accuracy_fn(model, images, labels):\n",
    "    logits = model(images, training=False)    # False: dropout off\n",
    "    prediction = tf.equal(tf.argmax(labels, -1), tf.argmax(logits, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f604336",
   "metadata": {},
   "source": [
    "### Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfa93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten() :\n",
    "    return tf.keras.layers.Flatten()\n",
    "\n",
    "# to use fully-connected layer\n",
    "def dense(label_dim, weight_init) :\n",
    "    return tf.keras.layers.Dense(units=label_dim, use_bias=True, kernel_initializer=weight_init)\n",
    "\n",
    "# Sigmoid func\n",
    "def sigmoid() :\n",
    "    return tf.keras.layers.Activation(tf.keras.activations.sigmoid)\n",
    "\n",
    "# ReLU func\n",
    "def relu() :\n",
    "    return tf.keras.layers.Activation(tf.keras.activations.relu)\n",
    "\n",
    "# Dropout\n",
    "def dropout(rate):\n",
    "    return tf.keras.layers.Dropout(rate)\n",
    "\n",
    "# Batch Normalization\n",
    "def batch_norm():\n",
    "    return tf.keras.layers.BatchNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92409207",
   "metadata": {},
   "source": [
    "### Create model (class version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b40238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class create_model_class(tf.keras.Model):\n",
    "#     def __init__(self, label_dim):\n",
    "#         super(create_model_class, self).__init__()\n",
    "#         weight_init = tf.keras.initializers.RandomNormal()\n",
    "#         self.model = tf.keras.Sequential()\n",
    "        \n",
    "#         self.model.add(flatten())    # [N, 28, 28, 1] -> [N, 784]\n",
    "\n",
    "#         for i in range(2):\n",
    "#             # [N, 784] -> [N, 256] -> [N, 256]\n",
    "#             self.model.add(dense(256, weight_init))\n",
    "#             self.model.add(sigmoid())\n",
    "#             self.model.add(dropout(rate=0.5))    # dropout\n",
    "\n",
    "#         self.model.add(dense(label_dim, weight_init))    # [N, 256] -> [N, 10]\n",
    "\n",
    "#     def call(self, x, training=None, mask=None):\n",
    "#         x = self.model(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a6cf2",
   "metadata": {},
   "source": [
    "### Create model (function version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e047df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_function(label_dim) :\n",
    "#     weight_init = tf.keras.initializers.RandomNormal()\n",
    "    weight_init = tf.keras.initializers.glorot_uniform()    # Xavier initialization\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(flatten())    # [N, 28, 28, 1] -> [N, 784]\n",
    "\n",
    "    for i in range(2) :\n",
    "        # [N, 784] -> [N, 256] -> [N, 256]\n",
    "        '''layer -> normalization -> activation 순서가 일반적'''\n",
    "        model.add(dense(256, weight_init))\n",
    "#         model.add(batch_norm())\n",
    "#         model.add(sigmoid())\n",
    "        model.add(relu())\n",
    "#         model.add(dropout(rate=0.5))\n",
    "\n",
    "    model.add(dense(label_dim, weight_init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa44cb9d",
   "metadata": {},
   "source": [
    "### Define data & hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4614941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dataset \"\"\"\n",
    "train_x, train_y, test_x, test_y = load_mnist()\n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "training_epochs = 1\n",
    "training_iterations = len(train_x) // batch_size    # 60,000 / 128 = 468.75\n",
    "\n",
    "label_dim = 10\n",
    "\n",
    "train_flag = True\n",
    "\n",
    "\"\"\" Graph Input using Dataset API \"\"\"\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=batch_size).\\\n",
    "    batch(batch_size, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=len(test_x)).\\\n",
    "    batch(len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd147895",
   "metadata": {},
   "source": [
    "### Define model & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b42a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "network = create_model_function(label_dim)\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5816c1",
   "metadata": {},
   "source": [
    "### Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f68c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = 'checkpoints'\n",
    "# logs_dir = 'logs'\n",
    "# model_dir = 'nn_softmax'\n",
    "\n",
    "# checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "# check_folder(checkpoint_dir)\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, model_dir)\n",
    "# logs_dir = os.path.join(logs_dir, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe1d74",
   "metadata": {},
   "source": [
    "### Restore checkpoint & start train or test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f4cb5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0] [    0/  468] time: 0.50, train_loss: 2.0879, train_accuracy: 0.4297, test_Accuracy: 0.2652\n",
      "Epoch: [ 0] [   10/  468] time: 1.74, train_loss: 0.9723, train_accuracy: 0.7812, test_Accuracy: 0.7719\n",
      "Epoch: [ 0] [   20/  468] time: 2.89, train_loss: 0.5492, train_accuracy: 0.8203, test_Accuracy: 0.8416\n",
      "Epoch: [ 0] [   30/  468] time: 4.05, train_loss: 0.4106, train_accuracy: 0.8906, test_Accuracy: 0.8788\n",
      "Epoch: [ 0] [   40/  468] time: 5.18, train_loss: 0.4604, train_accuracy: 0.8359, test_Accuracy: 0.8864\n",
      "Epoch: [ 0] [   50/  468] time: 6.30, train_loss: 0.3938, train_accuracy: 0.8750, test_Accuracy: 0.8934\n",
      "Epoch: [ 0] [   60/  468] time: 7.46, train_loss: 0.3481, train_accuracy: 0.9219, test_Accuracy: 0.8986\n",
      "Epoch: [ 0] [   70/  468] time: 8.60, train_loss: 0.2592, train_accuracy: 0.9297, test_Accuracy: 0.9094\n",
      "Epoch: [ 0] [   80/  468] time: 9.74, train_loss: 0.2235, train_accuracy: 0.9375, test_Accuracy: 0.9120\n",
      "Epoch: [ 0] [   90/  468] time: 10.88, train_loss: 0.2992, train_accuracy: 0.9219, test_Accuracy: 0.9213\n",
      "Epoch: [ 0] [  100/  468] time: 12.03, train_loss: 0.2856, train_accuracy: 0.8984, test_Accuracy: 0.9309\n",
      "Epoch: [ 0] [  110/  468] time: 13.22, train_loss: 0.2616, train_accuracy: 0.9219, test_Accuracy: 0.9272\n",
      "Epoch: [ 0] [  120/  468] time: 14.36, train_loss: 0.2390, train_accuracy: 0.9453, test_Accuracy: 0.9272\n",
      "Epoch: [ 0] [  130/  468] time: 15.50, train_loss: 0.3026, train_accuracy: 0.9375, test_Accuracy: 0.9263\n",
      "Epoch: [ 0] [  140/  468] time: 16.64, train_loss: 0.2487, train_accuracy: 0.9297, test_Accuracy: 0.9292\n",
      "Epoch: [ 0] [  150/  468] time: 17.78, train_loss: 0.2033, train_accuracy: 0.9297, test_Accuracy: 0.9323\n",
      "Epoch: [ 0] [  160/  468] time: 18.92, train_loss: 0.1892, train_accuracy: 0.9688, test_Accuracy: 0.9364\n",
      "Epoch: [ 0] [  170/  468] time: 20.05, train_loss: 0.1540, train_accuracy: 0.9531, test_Accuracy: 0.9401\n",
      "Epoch: [ 0] [  180/  468] time: 21.18, train_loss: 0.1708, train_accuracy: 0.9531, test_Accuracy: 0.9407\n",
      "Epoch: [ 0] [  190/  468] time: 22.32, train_loss: 0.1303, train_accuracy: 0.9688, test_Accuracy: 0.9451\n",
      "Epoch: [ 0] [  200/  468] time: 23.46, train_loss: 0.1214, train_accuracy: 0.9609, test_Accuracy: 0.9473\n",
      "Epoch: [ 0] [  210/  468] time: 24.61, train_loss: 0.1131, train_accuracy: 0.9688, test_Accuracy: 0.9493\n",
      "Epoch: [ 0] [  220/  468] time: 25.75, train_loss: 0.1728, train_accuracy: 0.9688, test_Accuracy: 0.9470\n",
      "Epoch: [ 0] [  230/  468] time: 26.88, train_loss: 0.1363, train_accuracy: 0.9688, test_Accuracy: 0.9414\n",
      "Epoch: [ 0] [  240/  468] time: 28.02, train_loss: 0.2279, train_accuracy: 0.9375, test_Accuracy: 0.9369\n",
      "Epoch: [ 0] [  250/  468] time: 29.15, train_loss: 0.2135, train_accuracy: 0.9297, test_Accuracy: 0.9497\n",
      "Epoch: [ 0] [  260/  468] time: 30.29, train_loss: 0.1117, train_accuracy: 0.9531, test_Accuracy: 0.9499\n",
      "Epoch: [ 0] [  270/  468] time: 31.46, train_loss: 0.2638, train_accuracy: 0.9297, test_Accuracy: 0.9540\n",
      "Epoch: [ 0] [  280/  468] time: 32.64, train_loss: 0.2042, train_accuracy: 0.9219, test_Accuracy: 0.9548\n",
      "Epoch: [ 0] [  290/  468] time: 33.76, train_loss: 0.2955, train_accuracy: 0.9062, test_Accuracy: 0.9522\n",
      "Epoch: [ 0] [  300/  468] time: 34.93, train_loss: 0.0856, train_accuracy: 0.9844, test_Accuracy: 0.9547\n",
      "Epoch: [ 0] [  310/  468] time: 36.07, train_loss: 0.1411, train_accuracy: 0.9609, test_Accuracy: 0.9569\n",
      "Epoch: [ 0] [  320/  468] time: 37.26, train_loss: 0.2869, train_accuracy: 0.9219, test_Accuracy: 0.9584\n",
      "Epoch: [ 0] [  330/  468] time: 38.38, train_loss: 0.1468, train_accuracy: 0.9609, test_Accuracy: 0.9491\n",
      "Epoch: [ 0] [  340/  468] time: 39.53, train_loss: 0.1556, train_accuracy: 0.9609, test_Accuracy: 0.9533\n",
      "Epoch: [ 0] [  350/  468] time: 40.65, train_loss: 0.1300, train_accuracy: 0.9453, test_Accuracy: 0.9521\n",
      "Epoch: [ 0] [  360/  468] time: 41.81, train_loss: 0.1564, train_accuracy: 0.9531, test_Accuracy: 0.9559\n",
      "Epoch: [ 0] [  370/  468] time: 42.95, train_loss: 0.1098, train_accuracy: 0.9609, test_Accuracy: 0.9546\n",
      "Epoch: [ 0] [  380/  468] time: 44.10, train_loss: 0.1552, train_accuracy: 0.9453, test_Accuracy: 0.9548\n",
      "Epoch: [ 0] [  390/  468] time: 45.28, train_loss: 0.1510, train_accuracy: 0.9688, test_Accuracy: 0.9595\n",
      "Epoch: [ 0] [  400/  468] time: 46.40, train_loss: 0.1940, train_accuracy: 0.9453, test_Accuracy: 0.9629\n",
      "Epoch: [ 0] [  410/  468] time: 47.57, train_loss: 0.1628, train_accuracy: 0.9688, test_Accuracy: 0.9636\n",
      "Epoch: [ 0] [  420/  468] time: 48.70, train_loss: 0.1272, train_accuracy: 0.9609, test_Accuracy: 0.9594\n",
      "Epoch: [ 0] [  430/  468] time: 49.85, train_loss: 0.0964, train_accuracy: 0.9609, test_Accuracy: 0.9607\n",
      "Epoch: [ 0] [  440/  468] time: 50.98, train_loss: 0.1465, train_accuracy: 0.9531, test_Accuracy: 0.9565\n",
      "Epoch: [ 0] [  450/  468] time: 52.16, train_loss: 0.1856, train_accuracy: 0.9531, test_Accuracy: 0.9577\n",
      "Epoch: [ 0] [  460/  468] time: 53.32, train_loss: 0.1173, train_accuracy: 0.9609, test_Accuracy: 0.9625\n",
      "Epoch: [ 0] [  467/  468] time: 54.13, train_loss: 0.1295, train_accuracy: 0.9609, test_Accuracy: 0.9644\n"
     ]
    }
   ],
   "source": [
    "'''NO CHECKPOINT'''\n",
    "\n",
    "start_epoch = 0\n",
    "start_iteration = 0\n",
    "\n",
    "start_time = time()\n",
    "for epoch in range(start_epoch, training_epochs):\n",
    "    for idx, (train_input, train_label) in enumerate(train_dataset):            \n",
    "        grads = grad(network, train_input, train_label)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables))\n",
    "\n",
    "        train_loss = loss_fn(network, train_input, train_label)\n",
    "        train_accuracy = accuracy_fn(network, train_input, train_label)\n",
    "\n",
    "        for test_input, test_label in test_dataset:                \n",
    "            test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "        \n",
    "#         print(\n",
    "#             \"Epoch: [%2d] [%5d/%5d] time: %4.2f, train_loss: %.4f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "#             % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy, test_accuracy))\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(\n",
    "                \"Epoch: [%2d] [%5d/%5d] time: %4.2f, train_loss: %.4f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "                % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy, test_accuracy))\n",
    "        elif idx == training_iterations - 1:\n",
    "            print(\n",
    "                \"Epoch: [%2d] [%5d/%5d] time: %4.2f, train_loss: %.4f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "                % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bafabf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nif train_flag :\\n\\n    checkpoint = tf.train.Checkpoint(dnn=network)\\n\\n    # create writer for tensorboard\\n    summary_writer = tf.summary.create_file_writer(logdir=logs_dir)\\n    start_time = time()\\n\\n    # restore check-point if it exits\\n    could_load, checkpoint_counter = load(network, checkpoint_dir)    \\n\\n    if could_load:\\n        start_epoch = (int)(checkpoint_counter / training_iterations)        \\n        counter = checkpoint_counter        \\n        print(\" [*] Load SUCCESS\")\\n    else:\\n        start_epoch = 0\\n        start_iteration = 0\\n        counter = 0\\n        print(\" [!] Load failed...\")\\n\\n    # train phase\\n    with summary_writer.as_default():  # for tensorboard\\n        for epoch in range(start_epoch, training_epochs):\\n            for idx, (train_input, train_label) in enumerate(train_dataset):            \\n                grads = grad(network, train_input, train_label)\\n                optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables))\\n\\n                train_loss = loss_fn(network, train_input, train_label)\\n                train_accuracy = accuracy_fn(network, train_input, train_label)\\n                \\n                for test_input, test_label in test_dataset:                \\n                    test_accuracy = accuracy_fn(network, test_input, test_label)\\n\\n                tf.summary.scalar(name=\\'train_loss\\', data=train_loss, step=counter)\\n                tf.summary.scalar(name=\\'train_accuracy\\', data=train_accuracy, step=counter)\\n                tf.summary.scalar(name=\\'test_accuracy\\', data=test_accuracy, step=counter)\\n\\n                print(\\n                    \"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_loss: %.8f, train_accuracy: %.4f, test_Accuracy: %.4f\"                     % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy,\\n                       test_accuracy))\\n                counter += 1                \\n        checkpoint.save(file_prefix=checkpoint_prefix + \\'-{}\\'.format(counter))\\n        \\n# test phase      \\nelse :\\n    _, _ = load(network, checkpoint_dir)\\n    for test_input, test_label in test_dataset:    \\n        test_accuracy = accuracy_fn(network, test_input, test_label)\\n\\n    print(\"test_Accuracy: %.4f\" % (test_accuracy))\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ORIGINAL SOURCE CODE\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if train_flag :\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(dnn=network)\n",
    "\n",
    "    # create writer for tensorboard\n",
    "    summary_writer = tf.summary.create_file_writer(logdir=logs_dir)\n",
    "    start_time = time()\n",
    "\n",
    "    # restore check-point if it exits\n",
    "    could_load, checkpoint_counter = load(network, checkpoint_dir)    \n",
    "\n",
    "    if could_load:\n",
    "        start_epoch = (int)(checkpoint_counter / training_iterations)        \n",
    "        counter = checkpoint_counter        \n",
    "        print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        start_iteration = 0\n",
    "        counter = 0\n",
    "        print(\" [!] Load failed...\")\n",
    "\n",
    "    # train phase\n",
    "    with summary_writer.as_default():  # for tensorboard\n",
    "        for epoch in range(start_epoch, training_epochs):\n",
    "            for idx, (train_input, train_label) in enumerate(train_dataset):            \n",
    "                grads = grad(network, train_input, train_label)\n",
    "                optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables))\n",
    "\n",
    "                train_loss = loss_fn(network, train_input, train_label)\n",
    "                train_accuracy = accuracy_fn(network, train_input, train_label)\n",
    "                \n",
    "                for test_input, test_label in test_dataset:                \n",
    "                    test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "\n",
    "                tf.summary.scalar(name='train_loss', data=train_loss, step=counter)\n",
    "                tf.summary.scalar(name='train_accuracy', data=train_accuracy, step=counter)\n",
    "                tf.summary.scalar(name='test_accuracy', data=test_accuracy, step=counter)\n",
    "\n",
    "                print(\n",
    "                    \"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_loss: %.8f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "                    % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy,\n",
    "                       test_accuracy))\n",
    "                counter += 1                \n",
    "        checkpoint.save(file_prefix=checkpoint_prefix + '-{}'.format(counter))\n",
    "        \n",
    "# test phase      \n",
    "else :\n",
    "    _, _ = load(network, checkpoint_dir)\n",
    "    for test_input, test_label in test_dataset:    \n",
    "        test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "\n",
    "    print(\"test_Accuracy: %.4f\" % (test_accuracy))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770b4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
